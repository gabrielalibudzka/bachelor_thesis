{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hk/0nprt9x92231fsxzv3sf0jph0000gn/T/ipykernel_2530/1278007067.py:1: DtypeWarning: Columns (1,2,3,6,7,8,10,11,14,15,16,17,18,22,24,25,26,29,34,35,36,38,48,50,51,57,59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"data/csv_result-4year.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/csv_result-4year.csv\")\n",
    "df = df.loc[:, df.columns!='id'].replace('?', np.nan)\n",
    "top = 0.887765522875817\n",
    "df = df.loc[:, ~df.columns.isin(['Attr37', 'Attr27', 'Attr24'])].dropna()\n",
    "positive_class_rate = df['class'].sum()/len(df)\n",
    "df = df.astype(float)\n",
    "df.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Information Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 43\n",
    "RATIO = 1.0\n",
    "K_FOLD = 5\n",
    "ros = RandomOverSampler(random_state=RANDOM_STATE, sampling_strategy = RATIO)\n",
    "lr = LogisticRegression(penalty='none')\n",
    "pipeline = make_pipeline(ros, lr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, ~df.columns.isin(['class'])]\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Attr1</th>\n",
       "      <th>Attr2</th>\n",
       "      <th>Attr3</th>\n",
       "      <th>Attr4</th>\n",
       "      <th>Attr5</th>\n",
       "      <th>Attr6</th>\n",
       "      <th>Attr7</th>\n",
       "      <th>Attr8</th>\n",
       "      <th>Attr9</th>\n",
       "      <th>...</th>\n",
       "      <th>Attr56</th>\n",
       "      <th>Attr57</th>\n",
       "      <th>Attr58</th>\n",
       "      <th>Attr59</th>\n",
       "      <th>Attr60</th>\n",
       "      <th>Attr61</th>\n",
       "      <th>Attr62</th>\n",
       "      <th>Attr63</th>\n",
       "      <th>Attr64</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.159290</td>\n",
       "      <td>0.46240</td>\n",
       "      <td>0.077730</td>\n",
       "      <td>1.16830</td>\n",
       "      <td>-44.8530</td>\n",
       "      <td>0.467020</td>\n",
       "      <td>0.189480</td>\n",
       "      <td>0.82895</td>\n",
       "      <td>1.12230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.108990</td>\n",
       "      <td>0.415570</td>\n",
       "      <td>0.89101</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>7.7928</td>\n",
       "      <td>4.9914</td>\n",
       "      <td>119.810</td>\n",
       "      <td>3.0465</td>\n",
       "      <td>3.05600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-0.127430</td>\n",
       "      <td>0.46243</td>\n",
       "      <td>0.269170</td>\n",
       "      <td>1.75170</td>\n",
       "      <td>7.5970</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>-0.127430</td>\n",
       "      <td>1.16250</td>\n",
       "      <td>1.29440</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089372</td>\n",
       "      <td>-0.237040</td>\n",
       "      <td>1.06250</td>\n",
       "      <td>0.150410</td>\n",
       "      <td>5.4327</td>\n",
       "      <td>3.4629</td>\n",
       "      <td>100.970</td>\n",
       "      <td>3.6150</td>\n",
       "      <td>3.47250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.070488</td>\n",
       "      <td>0.23570</td>\n",
       "      <td>0.527810</td>\n",
       "      <td>3.23930</td>\n",
       "      <td>125.6800</td>\n",
       "      <td>0.163670</td>\n",
       "      <td>0.086895</td>\n",
       "      <td>2.87180</td>\n",
       "      <td>1.05740</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054286</td>\n",
       "      <td>0.104130</td>\n",
       "      <td>0.94571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.1070</td>\n",
       "      <td>3.3808</td>\n",
       "      <td>76.076</td>\n",
       "      <td>4.7978</td>\n",
       "      <td>4.78180</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.136760</td>\n",
       "      <td>0.40538</td>\n",
       "      <td>0.315430</td>\n",
       "      <td>1.87050</td>\n",
       "      <td>19.1150</td>\n",
       "      <td>0.504970</td>\n",
       "      <td>0.136760</td>\n",
       "      <td>1.45390</td>\n",
       "      <td>1.11440</td>\n",
       "      <td>...</td>\n",
       "      <td>0.102630</td>\n",
       "      <td>0.232030</td>\n",
       "      <td>0.89737</td>\n",
       "      <td>0.073024</td>\n",
       "      <td>6.1384</td>\n",
       "      <td>4.2241</td>\n",
       "      <td>88.299</td>\n",
       "      <td>4.1337</td>\n",
       "      <td>4.64840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.110080</td>\n",
       "      <td>0.69793</td>\n",
       "      <td>0.188780</td>\n",
       "      <td>1.27130</td>\n",
       "      <td>-15.3440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.110080</td>\n",
       "      <td>0.43282</td>\n",
       "      <td>1.73500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.439880</td>\n",
       "      <td>-0.364400</td>\n",
       "      <td>0.57153</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.8010</td>\n",
       "      <td>2.7925</td>\n",
       "      <td>146.390</td>\n",
       "      <td>2.4934</td>\n",
       "      <td>15.03600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8688</th>\n",
       "      <td>9787</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.54949</td>\n",
       "      <td>0.192810</td>\n",
       "      <td>1.38990</td>\n",
       "      <td>-39.0640</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.78627</td>\n",
       "      <td>0.97093</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029937</td>\n",
       "      <td>0.010823</td>\n",
       "      <td>1.02990</td>\n",
       "      <td>0.127190</td>\n",
       "      <td>3.8159</td>\n",
       "      <td>3.3892</td>\n",
       "      <td>146.860</td>\n",
       "      <td>2.4854</td>\n",
       "      <td>3.93150</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8689</th>\n",
       "      <td>9788</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>0.60748</td>\n",
       "      <td>-0.029762</td>\n",
       "      <td>0.90591</td>\n",
       "      <td>-20.9230</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>-0.027610</td>\n",
       "      <td>0.55161</td>\n",
       "      <td>1.00730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007198</td>\n",
       "      <td>-0.082395</td>\n",
       "      <td>0.99280</td>\n",
       "      <td>0.868910</td>\n",
       "      <td>23.0280</td>\n",
       "      <td>27.1360</td>\n",
       "      <td>37.047</td>\n",
       "      <td>9.8523</td>\n",
       "      <td>4.36810</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8690</th>\n",
       "      <td>9789</td>\n",
       "      <td>-0.238290</td>\n",
       "      <td>0.62708</td>\n",
       "      <td>0.090374</td>\n",
       "      <td>1.61250</td>\n",
       "      <td>-1.0692</td>\n",
       "      <td>-0.238290</td>\n",
       "      <td>-0.240360</td>\n",
       "      <td>0.28322</td>\n",
       "      <td>0.80307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245220</td>\n",
       "      <td>-1.341700</td>\n",
       "      <td>1.24520</td>\n",
       "      <td>2.700100</td>\n",
       "      <td>6.5694</td>\n",
       "      <td>4.1781</td>\n",
       "      <td>88.883</td>\n",
       "      <td>4.1065</td>\n",
       "      <td>0.79501</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8691</th>\n",
       "      <td>9790</td>\n",
       "      <td>0.097188</td>\n",
       "      <td>0.75300</td>\n",
       "      <td>-0.327680</td>\n",
       "      <td>0.43850</td>\n",
       "      <td>-214.2400</td>\n",
       "      <td>-0.331300</td>\n",
       "      <td>0.104280</td>\n",
       "      <td>0.32803</td>\n",
       "      <td>0.98145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288240</td>\n",
       "      <td>0.393470</td>\n",
       "      <td>0.68127</td>\n",
       "      <td>0.508850</td>\n",
       "      <td>4.3246</td>\n",
       "      <td>35.5030</td>\n",
       "      <td>217.030</td>\n",
       "      <td>1.6818</td>\n",
       "      <td>1.31910</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8692</th>\n",
       "      <td>9791</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.48678</td>\n",
       "      <td>0.148940</td>\n",
       "      <td>1.30670</td>\n",
       "      <td>-24.2820</td>\n",
       "      <td>0.021416</td>\n",
       "      <td>0.027253</td>\n",
       "      <td>1.05320</td>\n",
       "      <td>1.00140</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.041773</td>\n",
       "      <td>0.99861</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>6.7582</td>\n",
       "      <td>4.9171</td>\n",
       "      <td>98.421</td>\n",
       "      <td>3.7085</td>\n",
       "      <td>4.92950</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8693 rows Ã— 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index     Attr1    Attr2     Attr3    Attr4     Attr5     Attr6  \\\n",
       "0         0  0.159290  0.46240  0.077730  1.16830  -44.8530  0.467020   \n",
       "1         1 -0.127430  0.46243  0.269170  1.75170    7.5970  0.000925   \n",
       "2         2  0.070488  0.23570  0.527810  3.23930  125.6800  0.163670   \n",
       "3         3  0.136760  0.40538  0.315430  1.87050   19.1150  0.504970   \n",
       "4         4 -0.110080  0.69793  0.188780  1.27130  -15.3440  0.000000   \n",
       "...     ...       ...      ...       ...      ...       ...       ...   \n",
       "8688   9787  0.004676  0.54949  0.192810  1.38990  -39.0640  0.004676   \n",
       "8689   9788 -0.027610  0.60748 -0.029762  0.90591  -20.9230 -0.027610   \n",
       "8690   9789 -0.238290  0.62708  0.090374  1.61250   -1.0692 -0.238290   \n",
       "8691   9790  0.097188  0.75300 -0.327680  0.43850 -214.2400 -0.331300   \n",
       "8692   9791  0.021416  0.48678  0.148940  1.30670  -24.2820  0.021416   \n",
       "\n",
       "         Attr7    Attr8    Attr9  ...    Attr56    Attr57   Attr58    Attr59  \\\n",
       "0     0.189480  0.82895  1.12230  ...  0.108990  0.415570  0.89101  0.001422   \n",
       "1    -0.127430  1.16250  1.29440  ... -0.089372 -0.237040  1.06250  0.150410   \n",
       "2     0.086895  2.87180  1.05740  ...  0.054286  0.104130  0.94571  0.000000   \n",
       "3     0.136760  1.45390  1.11440  ...  0.102630  0.232030  0.89737  0.073024   \n",
       "4    -0.110080  0.43282  1.73500  ...  0.439880 -0.364400  0.57153  0.000000   \n",
       "...        ...      ...      ...  ...       ...       ...      ...       ...   \n",
       "8688  0.013002  0.78627  0.97093  ... -0.029937  0.010823  1.02990  0.127190   \n",
       "8689 -0.027610  0.55161  1.00730  ...  0.007198 -0.082395  0.99280  0.868910   \n",
       "8690 -0.240360  0.28322  0.80307  ... -0.245220 -1.341700  1.24520  2.700100   \n",
       "8691  0.104280  0.32803  0.98145  ...  0.288240  0.393470  0.68127  0.508850   \n",
       "8692  0.027253  1.05320  1.00140  ...  0.001393  0.041773  0.99861  0.002146   \n",
       "\n",
       "       Attr60   Attr61   Attr62  Attr63    Attr64  class  \n",
       "0      7.7928   4.9914  119.810  3.0465   3.05600    0.0  \n",
       "1      5.4327   3.4629  100.970  3.6150   3.47250    0.0  \n",
       "2      7.1070   3.3808   76.076  4.7978   4.78180    0.0  \n",
       "3      6.1384   4.2241   88.299  4.1337   4.64840    0.0  \n",
       "4     18.8010   2.7925  146.390  2.4934  15.03600    0.0  \n",
       "...       ...      ...      ...     ...       ...    ...  \n",
       "8688   3.8159   3.3892  146.860  2.4854   3.93150    1.0  \n",
       "8689  23.0280  27.1360   37.047  9.8523   4.36810    1.0  \n",
       "8690   6.5694   4.1781   88.883  4.1065   0.79501    1.0  \n",
       "8691   4.3246  35.5030  217.030  1.6818   1.31910    1.0  \n",
       "8692   6.7582   4.9171   98.421  3.7085   4.92950    1.0  \n",
       "\n",
       "[8693 rows x 63 columns]"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrielalibudzka/.pyenv/versions/3.9.13/envs/bachelor_thesis-3.9.13/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gabrielalibudzka/.pyenv/versions/3.9.13/envs/bachelor_thesis-3.9.13/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gabrielalibudzka/.pyenv/versions/3.9.13/envs/bachelor_thesis-3.9.13/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gabrielalibudzka/.pyenv/versions/3.9.13/envs/bachelor_thesis-3.9.13/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/gabrielalibudzka/.pyenv/versions/3.9.13/envs/bachelor_thesis-3.9.13/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "auc_roc = []\n",
    "kf = StratifiedKFold(n_splits=K_FOLD)\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    y_pred = pipeline.fit(X_train, y_train).predict_proba(X_test)[:,1]\n",
    "\n",
    "    auc_roc.append(roc_auc_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "#print('ROC_AUC = {} +/- {}'.format(np.round(mean(auc_roc),4), np.round(stdev(auc_roc),4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('bachelor_thesis-3.9.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "516fe53d8155453fc21db612f4db749473e340bfc720fb6dc592b0a5d8296f4d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
